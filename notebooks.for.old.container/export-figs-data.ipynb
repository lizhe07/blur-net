{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ed962bc-1346-49a8-99cf-1bee6a6ad49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b647c4e-09a6-4256-bc2b-2bcd29acf0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from roarena.corruption import CorruptionJob\n",
    "from roarena.corruption import CORRUPTIONS\n",
    "\n",
    "job = CorruptionJob('store', 'datasets', readonly=True)\n",
    "\n",
    "for animal in ['mouse', 'monkey']:\n",
    "    if animal=='mouse':\n",
    "        model_paths = {\n",
    "            'base': 'store/models/CIFAR10-G_Baseline.pt',\n",
    "            'neural': 'store/models/CIFAR10-G_Mouse.pt',\n",
    "        }\n",
    "    if animal=='monkey':\n",
    "        model_paths = {\n",
    "            'base': 'store/models/VGG19_Baseline.pt',\n",
    "            'neural': 'store/models/VGG19_MTL.pt',\n",
    "        }\n",
    "    \n",
    "    accs = {}\n",
    "    for label, model_path in model_paths.items():\n",
    "        saved = torch.load(model_path)\n",
    "        accs[label] = {0: saved['acc']}\n",
    "        for severity in range(1, 6):\n",
    "            accs[label][severity] = []\n",
    "            for corruption in CORRUPTIONS:\n",
    "                config = {\n",
    "                    'model_pth': model_path,\n",
    "                    'severity': severity,\n",
    "                    'corruption': corruption,\n",
    "                }\n",
    "                key = job.configs.get_key(config)\n",
    "                accs[label][severity].append(job.previews[key]['acc'])\n",
    "    torch.save(accs, f'store/figs-data/{animal}_corruption.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1787cf6b-4bb1-4e98-bfca-2d7f63b615a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/foolbox/foolbox/attacks/brendel_bethge.py:624: UserWarning: At the first initialisation the optimizer needs to be compiled. This may take between 20 to 60 seconds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gathering advs for store/models/CIFAR10-G_Baseline.pt...\n",
      "10 batches loaded\n",
      "20 batches loaded\n",
      "30 batches loaded\n",
      "40 batches loaded\n",
      "50 batches loaded\n",
      "gathering advs for store/models/CIFAR10-G_Mouse.pt...\n",
      "10 batches loaded\n",
      "20 batches loaded\n",
      "30 batches loaded\n",
      "40 batches loaded\n",
      "50 batches loaded\n",
      "gathering advs for store/models/VGG19_Baseline.pt...\n",
      "10 batches loaded\n",
      "20 batches loaded\n",
      "30 batches loaded\n",
      "40 batches loaded\n",
      "50 batches loaded\n",
      "gathering advs for store/models/VGG19_MTL.pt...\n",
      "10 batches loaded\n",
      "20 batches loaded\n",
      "30 batches loaded\n",
      "40 batches loaded\n",
      "50 batches loaded\n"
     ]
    }
   ],
   "source": [
    "from roarena.attack import AttackJob\n",
    "from jarvis.vision import prepare_datasets\n",
    "\n",
    "job = AttackJob('store', 'datasets', readonly=True)\n",
    "\n",
    "metric = 'LI'\n",
    "targeted = True\n",
    "\n",
    "for animal in ['mouse', 'monkey']:\n",
    "    if animal=='mouse':\n",
    "        model_paths = {\n",
    "            'base': 'store/models/CIFAR10-G_Baseline.pt',\n",
    "            'neural': 'store/models/CIFAR10-G_Mouse.pt',\n",
    "        }\n",
    "        dataset = prepare_datasets('CIFAR10-Gray', 'datasets')\n",
    "    if animal=='monkey':\n",
    "        model_paths = {\n",
    "            'base': 'store/models/VGG19_Baseline.pt',\n",
    "            'neural': 'store/models/VGG19_MTL.pt',\n",
    "        }\n",
    "        dataset = prepare_datasets('TinyImageNet-Gray', 'datasets')\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=20)\n",
    "\n",
    "    imgs, advs = {}, {}\n",
    "    for label, model_path in model_paths.items():\n",
    "        print(f'gathering advs for {model_path}...')\n",
    "        imgs[label] = []\n",
    "        advs[label] = []\n",
    "        for b_idx, (_imgs, _) in enumerate(loader):\n",
    "            if b_idx>=50:\n",
    "                break\n",
    "\n",
    "            cond = {\n",
    "                'model_pth': model_path,\n",
    "                'batch_idx': b_idx,\n",
    "                'metric': metric, 'targeted': targeted,\n",
    "                'name': 'BB',\n",
    "            }\n",
    "            _advs, _dists = [], []\n",
    "            for key, config in job.conditioned(cond):\n",
    "                result = job.results[key]\n",
    "                _advs.append(result['advs'])\n",
    "                _dists.append(result['dists'])\n",
    "            if len(_dists)==0:\n",
    "                continue\n",
    "            imgs[label].append(_imgs)\n",
    "\n",
    "            _advs = np.array(_advs)\n",
    "            _dists = np.array(_dists)\n",
    "            idxs = np.argmin(_dists, axis=0)\n",
    "            advs[label].append(np.array([_advs[idx, i] for i, idx in enumerate(idxs)]))\n",
    "\n",
    "            if (b_idx+1)%10==0:\n",
    "                print('{:2d} batches loaded'.format(b_idx+1))\n",
    "        imgs[label] = np.concatenate(imgs[label])\n",
    "        advs[label] = np.concatenate(advs[label])\n",
    "    torch.save({'imgs': imgs, 'advs': advs}, f'store/figs-data/{animal}_advs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4899a81-12d0-487c-88ee-ef3363e0f2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from roarena.einmon import EinMonJob\n",
    "from scipy.fft import fft2, ifft2\n",
    "\n",
    "job = EinMonJob('store', 'datasets', readonly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0409bec-e670-434d-8351-6ea4a73b76f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EinMon images for mouse saved.\n",
      "EinMon images for monkey saved.\n"
     ]
    }
   ],
   "source": [
    "for animal in ['mouse', 'monkey']:\n",
    "    if animal=='mouse':\n",
    "        dataset = job.prepare_dataset('CIFAR10-Gray', 0, 0)\n",
    "        idx = 5796\n",
    "    if animal=='monkey':\n",
    "        dataset = job.prepare_dataset('TinyImageNet-Gray', 0, 0)\n",
    "        idx = 7961\n",
    "    img_low, label_low = dataset.dataset[dataset.idxs_low[idx]]\n",
    "    img_high, label_high = dataset.dataset[dataset.idxs_high[idx]]\n",
    "\n",
    "    alphas = [0, 16, 32, 48, 64, 80]\n",
    "    imgs_low, imgs_high, imgs_mix = {}, {}, {}\n",
    "    for i, alpha in enumerate(alphas):\n",
    "        img_size = img_low.shape[1]\n",
    "        assert img_low.shape[2]==img_size\n",
    "        dx, dy = np.meshgrid(np.arange(img_size)/img_size, np.arange(img_size)/img_size)\n",
    "        dx = np.mod(dx+0.5, 1)-0.5\n",
    "        dy = np.mod(dy+0.5, 1)-0.5\n",
    "        mask = ((dx**2+dy**2)**0.5<=alpha/100*0.5).astype(float)\n",
    "\n",
    "        f_low = fft2(img_low.numpy())\n",
    "        f_high = fft2(img_high.numpy())\n",
    "        f_mix = f_low*mask+f_high*(1-mask)\n",
    "        img_mix = np.real(ifft2(f_mix))\n",
    "        img_mix = np.clip(img_mix, 0, 1)\n",
    "\n",
    "        imgs_low[alpha] = np.clip(np.real(ifft2(f_low*mask)), 0, 1)[0]\n",
    "        imgs_high[alpha] = np.clip(np.real(ifft2(f_high*(1-mask)))+img_high.mean().item(), 0, 1)\n",
    "        imgs_mix[alpha] = img_mix[0]\n",
    "    torch.save({\n",
    "        'imgs_low': imgs_low, 'imgs_high': imgs_high, 'imgs_mix': imgs_mix, 'alphas': alphas,\n",
    "    }, f'store/figs-data/{animal}_einmon_imgs.pt')\n",
    "    print(f'EinMon images for {animal} saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac4743d4-1c36-4f00-ba5c-c0709c5ffb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gathering ein-mon results for store/models/CIFAR10-G_Baseline.pt...\n",
      "gathering ein-mon results for store/models/CIFAR10-G_Mouse.pt...\n",
      "gathering ein-mon results for store/models/VGG19_Baseline.pt...\n",
      "gathering ein-mon results for store/models/VGG19_MTL.pt...\n"
     ]
    }
   ],
   "source": [
    "for animal in ['mouse', 'monkey']:\n",
    "    if animal=='mouse':\n",
    "        model_paths = {\n",
    "            'base': 'store/models/CIFAR10-G_Baseline.pt',\n",
    "            'neural': 'store/models/CIFAR10-G_Mouse.pt',\n",
    "        }\n",
    "    if animal=='monkey':\n",
    "        model_paths = {\n",
    "            'base': 'store/models/VGG19_Baseline.pt',\n",
    "            'neural': 'store/models/VGG19_MTL.pt',\n",
    "        }\n",
    "    \n",
    "    alphas = {}\n",
    "    accs_low, accs_high = {}, {}\n",
    "    for label, model_path in model_paths.items():\n",
    "        print(f'gathering ein-mon results for {model_path}...')\n",
    "        alphas[label] = set()\n",
    "        cond = {\n",
    "            'model_pth': model_path,\n",
    "        }\n",
    "        for _, config in job.conditioned(cond):\n",
    "            alphas[label].add(config['alpha'])\n",
    "        alphas[label] = sorted(list(alphas[label]))\n",
    "        accs_low[label], accs_high[label] = [], []\n",
    "        for alpha in alphas[label]:\n",
    "            cond['alpha'] = alpha\n",
    "            _accs_low, _accs_high = [], []\n",
    "            for key, _ in job.conditioned(cond):\n",
    "                result = job.results[key]\n",
    "                _accs_low.append(result['acc_low'])\n",
    "                _accs_high.append(result['acc_high'])\n",
    "            accs_low[label].append(_accs_low)\n",
    "            accs_high[label].append(_accs_high)\n",
    "    torch.save({\n",
    "        'alphas': alphas, 'accs_low': accs_low, 'accs_high': accs_high,\n",
    "    }, f'store/figs-data/{animal}_einmon_accs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "788762fe-d0fd-4d87-9c45-dadd50d56da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing log Fourier powers for store/models/CIFAR10_Baseline.pt...\n",
      "computing log Fourier powers for store/models/CIFAR10_Rebuffi21.pt...\n",
      "computing log Fourier powers for store/models/CIFAR10_Gowal20.pt...\n",
      "computing log Fourier powers for store/models/CIFAR10_Wu20.pt...\n",
      "computing log Fourier powers for store/models/CIFAR10_Zhang20.pt...\n",
      "computing log Fourier powers for store/models/CIFAR10_Carmon19.pt...\n",
      "computing log Fourier powers for store/models/CIFAR10_Sehwag20.pt...\n",
      "computing log Fourier powers for store/models/CIFAR10_Cui20.pt...\n",
      "computing log Fourier powers for store/models/CIFAR10_Hendrycks20.pt...\n",
      "computing log Fourier powers for store/models/CIFAR10_Kireev21.pt...\n",
      "computing log Fourier powers for store/models/CIFAR10_Blur.pt...\n",
      "computing log Fourier powers for store/models/C10R18-PC512.pt...\n"
     ]
    }
   ],
   "source": [
    "from numpy.fft import fft2, fftshift, fftfreq\n",
    "\n",
    "job = AttackJob('store', 'datasets', readonly=True)\n",
    "\n",
    "metric = 'LI'\n",
    "targeted = True\n",
    "\n",
    "dataset = prepare_datasets('CIFAR10', 'datasets')\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=20)\n",
    "\n",
    "model_paths = {\n",
    "    'Baseline': 'store/models/CIFAR10_Baseline.pt',\n",
    "    'Rebuffi21': 'store/models/CIFAR10_Rebuffi21.pt',\n",
    "    'Gowal20': 'store/models/CIFAR10_Gowal20.pt',\n",
    "    'Wu20': 'store/models/CIFAR10_Wu20.pt',\n",
    "    'Zhang20': 'store/models/CIFAR10_Zhang20.pt',\n",
    "    'Carmon19': 'store/models/CIFAR10_Carmon19.pt',\n",
    "    'Sehwag20': 'store/models/CIFAR10_Sehwag20.pt',\n",
    "    'Cui20': 'store/models/CIFAR10_Cui20.pt',\n",
    "    'Hendrycks20': 'store/models/CIFAR10_Hendrycks20.pt',\n",
    "    'Kireev21': 'store/models/CIFAR10_Kireev21.pt',\n",
    "    'Blur': 'store/models/CIFAR10_Blur.pt',\n",
    "    'PCA': 'store/models/C10R18-PC512.pt',\n",
    "}\n",
    "\n",
    "logps, dists = {}, {}\n",
    "for label, model_path in model_paths.items():\n",
    "    print(f'computing log Fourier powers for {model_path}...')\n",
    "    imgs = []\n",
    "    advs = []\n",
    "    for b_idx, (_imgs, _) in enumerate(loader):\n",
    "        if b_idx>=50:\n",
    "            break\n",
    "\n",
    "        cond = {\n",
    "            'model_pth': model_path,\n",
    "            'batch_idx': b_idx,\n",
    "            'metric': metric, 'targeted': targeted,\n",
    "            'name': 'BB',\n",
    "        }\n",
    "        _advs, _dists = [], []\n",
    "        for key, config in job.conditioned(cond):\n",
    "            result = job.results[key]\n",
    "            _advs.append(result['advs'])\n",
    "            _dists.append(result['dists'])\n",
    "        if len(_dists)==0:\n",
    "            continue\n",
    "        imgs.append(_imgs)\n",
    "\n",
    "        _advs = np.array(_advs)\n",
    "        _dists = np.array(_dists)\n",
    "        idxs = np.argmin(_dists, axis=0)\n",
    "        advs.append(np.array([_advs[idx, i] for i, idx in enumerate(idxs)]))\n",
    "    imgs = torch.cat(imgs).numpy()\n",
    "    advs = np.concatenate(advs)\n",
    "    \n",
    "    diffs = imgs-advs\n",
    "    dists[label] = diffs.max(axis=(1, 2, 3))\n",
    "    diffs -= diffs.mean(axis=(2, 3), keepdims=True)\n",
    "    powers = np.abs(fft2(diffs))**2\n",
    "    powers[..., 0, 0] = np.nan\n",
    "    logps[label] = fftshift(np.log(powers.mean(axis=(0, 1))))\n",
    "torch.save({\n",
    "    'logps': logps, 'dists': dists,\n",
    "}, f'store/figs-data/CIFAR10_advs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcebeb9d-9225-4a13-8e77-22103437e840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gathering ein-mon results for store/models/CIFAR10_Baseline.pt...\n",
      "gathering ein-mon results for store/models/CIFAR10_Rebuffi21.pt...\n",
      "gathering ein-mon results for store/models/CIFAR10_Gowal20.pt...\n",
      "gathering ein-mon results for store/models/CIFAR10_Wu20.pt...\n",
      "gathering ein-mon results for store/models/CIFAR10_Zhang20.pt...\n",
      "gathering ein-mon results for store/models/CIFAR10_Carmon19.pt...\n",
      "gathering ein-mon results for store/models/CIFAR10_Sehwag20.pt...\n",
      "gathering ein-mon results for store/models/CIFAR10_Cui20.pt...\n",
      "gathering ein-mon results for store/models/CIFAR10_Hendrycks20.pt...\n",
      "gathering ein-mon results for store/models/CIFAR10_Kireev21.pt...\n",
      "gathering ein-mon results for store/models/CIFAR10_Blur.pt...\n",
      "gathering ein-mon results for store/models/C10R18-PC512.pt...\n"
     ]
    }
   ],
   "source": [
    "e_job = EinMonJob('store', 'datasets', readonly=True)\n",
    "c_job = CorruptionJob('store', 'datasets', readonly=True)\n",
    "\n",
    "alphas = {}\n",
    "accs_low, accs_high = {}, {}\n",
    "accs = {}\n",
    "for label, model_path in model_paths.items():\n",
    "    print(f'gathering ein-mon results for {model_path}...')\n",
    "    alphas[label] = set()\n",
    "    cond = {\n",
    "        'model_pth': model_path,\n",
    "    }\n",
    "    for _, config in e_job.conditioned(cond):\n",
    "        alphas[label].add(config['alpha'])\n",
    "    alphas[label] = sorted(list(alphas[label]))\n",
    "    accs_low[label], accs_high[label] = [], []\n",
    "    for alpha in alphas[label]:\n",
    "        cond['alpha'] = alpha\n",
    "        _accs_low, _accs_high = [], []\n",
    "        for key, _ in e_job.conditioned(cond):\n",
    "            result = e_job.results[key]\n",
    "            _accs_low.append(result['acc_low'])\n",
    "            _accs_high.append(result['acc_high'])\n",
    "        accs_low[label].append(_accs_low)\n",
    "        accs_high[label].append(_accs_high)\n",
    "        \n",
    "    accs[label] = []\n",
    "    for severity in range(1, 6):\n",
    "        for corruption in CORRUPTIONS:\n",
    "            config = {\n",
    "                'model_pth': model_path,\n",
    "                'severity': severity,\n",
    "                'corruption': corruption,\n",
    "            }\n",
    "            key = c_job.configs.get_key(config)\n",
    "            accs[label].append(c_job.previews[key]['acc'])\n",
    "    accs[label] = np.mean(accs[label])\n",
    "torch.save({\n",
    "    'alphas': alphas, 'accs_low': accs_low, 'accs_high': accs_high, 'accs': accs,\n",
    "}, f'store/figs-data/CIFAR10_einmon_accs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8704136-4995-4de0-a9fe-d1fc02bab3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gathering ein-mon results for store/models/C10R18_10.pt...\n",
      "gathering ein-mon results for store/models/C10R18_11.pt...\n",
      "gathering ein-mon results for store/models/C10R18_12.pt...\n",
      "gathering ein-mon results for store/models/C10R18_13.pt...\n",
      "gathering ein-mon results for store/models/C10R18_14.pt...\n",
      "gathering ein-mon results for store/models/C10R18_15.pt...\n",
      "gathering ein-mon results for store/models/C10R18_16.pt...\n",
      "gathering ein-mon results for store/models/C10R18_17.pt...\n",
      "gathering ein-mon results for store/models/C10R18_18.pt...\n",
      "gathering ein-mon results for store/models/C10R18_19.pt...\n",
      "gathering ein-mon results for store/models/C10R18_20.pt...\n",
      "gathering ein-mon results for store/models/C10R18_25.pt...\n",
      "gathering ein-mon results for store/models/C10R18_30.pt...\n"
     ]
    }
   ],
   "source": [
    "alphas = {}\n",
    "accs_low, accs_high = {}, {}\n",
    "accs = {}\n",
    "for label in [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 25, 30]:\n",
    "    model_path = 'store/models/C10R18_{:02d}.pt'.format(label)\n",
    "    print(f'gathering ein-mon results for {model_path}...')\n",
    "    alphas[label] = set()\n",
    "    cond = {\n",
    "        'model_pth': model_path,\n",
    "    }\n",
    "    for _, config in e_job.conditioned(cond):\n",
    "        alphas[label].add(config['alpha'])\n",
    "    alphas[label] = sorted(list(alphas[label]))\n",
    "    accs_low[label], accs_high[label] = [], []\n",
    "    for alpha in alphas[label]:\n",
    "        cond['alpha'] = alpha\n",
    "        _accs_low, _accs_high = [], []\n",
    "        for key, _ in e_job.conditioned(cond):\n",
    "            result = e_job.results[key]\n",
    "            _accs_low.append(result['acc_low'])\n",
    "            _accs_high.append(result['acc_high'])\n",
    "        accs_low[label].append(_accs_low)\n",
    "        accs_high[label].append(_accs_high)\n",
    "        \n",
    "    accs[label] = []\n",
    "    for severity in range(1, 6):\n",
    "        for corruption in CORRUPTIONS:\n",
    "            config = {\n",
    "                'model_pth': model_path,\n",
    "                'severity': severity,\n",
    "                'corruption': corruption,\n",
    "            }\n",
    "            key = c_job.configs.get_key(config)\n",
    "            accs[label].append(c_job.previews[key]['acc'])\n",
    "    accs[label] = np.mean(accs[label])\n",
    "torch.save({\n",
    "    'alphas': alphas, 'accs_low': accs_low, 'accs_high': accs_high, 'accs': accs,\n",
    "}, f'store/figs-data/CIFAR10-blur_einmon_accs.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
